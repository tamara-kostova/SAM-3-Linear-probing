{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1LNnUKjUTRj3z95gyMq5sAlKt8Vrnc23y","authorship_tag":"ABX9TyNB5J83W93bGvBn3u8zXx+h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# SAM 3 Linearprobing"],"metadata":{"id":"GwoaBmSMKfRw"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"QeJxEAstKho_"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"njcR7QSOflHK","executionInfo":{"status":"ok","timestamp":1769496048836,"user_tz":-60,"elapsed":8756,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}},"outputId":"8ccb6a01-6be6-449b-c9a5-6b64515334e6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/facebookresearch/sam3.git /content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM\\ 3/sam3_repo\n"],"metadata":{"id":"yhW9CR0Rfy2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM\\ 3/sam3_repo\n","!pip install -e \".[notebooks]\"\n","%cd /content\n","!pip install -q supervision jupyter_bbox_widget  > /dev/null\n","!pip install triton decord  > /dev/null"],"metadata":{"id":"rZYY0saPAk_L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769496216825,"user_tz":-60,"elapsed":46921,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}},"outputId":"ba807c4d-b8b6-4bac-db05-f397c44be1fe"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM 3/sam3_repo\n","Obtaining file:///content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM%203/sam3_repo\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (1.0.24)\n","Requirement already satisfied: numpy<2,>=1.26 in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (1.26.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (4.67.1)\n","Requirement already satisfied: ftfy==6.1.1 in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (6.1.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (2025.11.3)\n","Requirement already satisfied: iopath>=0.1.10 in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (0.1.10)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (4.15.0)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (0.36.0)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.12/dist-packages (from ftfy==6.1.1->sam3==0.1.0) (0.2.14)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (3.10.0)\n","Requirement already satisfied: jupyter in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (1.1.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (6.5.7)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (7.7.1)\n","Requirement already satisfied: ipycanvas in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (0.14.3)\n","Requirement already satisfied: ipympl in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (0.10.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (2.0.11)\n","Requirement already satisfied: decord in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (0.6.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (4.11.0.86)\n","Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (0.8.1)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (0.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sam3==0.1.0) (1.6.1)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.10->sam3==0.1.0) (3.2.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->sam3==0.1.0) (2.9.0+cpu)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->sam3==0.1.0) (0.24.0+cpu)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->sam3==0.1.0) (6.0.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->sam3==0.1.0) (0.7.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->sam3==0.1.0) (3.20.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->sam3==0.1.0) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->sam3==0.1.0) (25.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->sam3==0.1.0) (2.32.4)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->sam3==0.1.0) (1.2.0)\n","Requirement already satisfied: pillow>=6.0 in /usr/local/lib/python3.12/dist-packages (from ipycanvas->sam3==0.1.0) (11.3.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->sam3==0.1.0) (6.17.1)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->sam3==0.1.0) (0.2.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->sam3==0.1.0) (5.7.1)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->sam3==0.1.0) (3.6.10)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->sam3==0.1.0) (7.34.0)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->sam3==0.1.0) (3.0.16)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->sam3==0.1.0) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->sam3==0.1.0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->sam3==0.1.0) (4.61.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->sam3==0.1.0) (1.4.9)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->sam3==0.1.0) (3.3.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->sam3==0.1.0) (2.9.0.post0)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.12/dist-packages (from jupyter->sam3==0.1.0) (6.6.3)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (from jupyter->sam3==0.1.0) (7.16.6)\n","Requirement already satisfied: jupyterlab in /usr/local/lib/python3.12/dist-packages (from jupyter->sam3==0.1.0) (4.5.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from notebook->sam3==0.1.0) (3.1.6)\n","Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from notebook->sam3==0.1.0) (6.5.1)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from notebook->sam3==0.1.0) (26.2.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook->sam3==0.1.0) (25.1.0)\n","Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.12/dist-packages (from notebook->sam3==0.1.0) (5.9.1)\n","Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.12/dist-packages (from notebook->sam3==0.1.0) (7.4.9)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (from notebook->sam3==0.1.0) (5.10.4)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.12/dist-packages (from notebook->sam3==0.1.0) (1.6.0)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook->sam3==0.1.0) (2.1.0)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook->sam3==0.1.0) (0.18.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook->sam3==0.1.0) (0.24.1)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook->sam3==0.1.0) (1.3.3)\n","Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->sam3==0.1.0) (1.16.3)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->sam3==0.1.0) (3.6.1)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->sam3==0.1.0) (2.37.2)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->sam3==0.1.0) (2026.1.14)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->sam3==0.1.0) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sam3==0.1.0) (1.5.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sam3==0.1.0) (3.6.0)\n","Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets->sam3==0.1.0) (1.8.15)\n","Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets->sam3==0.1.0) (0.2.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets->sam3==0.1.0) (5.9.5)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->sam3==0.1.0) (75.2.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->sam3==0.1.0) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->sam3==0.1.0) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->sam3==0.1.0) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->sam3==0.1.0) (3.0.52)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->sam3==0.1.0) (2.19.2)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->sam3==0.1.0) (0.2.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->sam3==0.1.0) (4.9.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client<8,>=5.3.4->notebook->sam3==0.1.0) (0.4)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.6.1->notebook->sam3==0.1.0) (4.5.1)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from nbclassic>=0.4.7->notebook->sam3==0.1.0) (0.2.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->sam3==0.1.0) (4.13.5)\n","Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->sam3==0.1.0) (6.3.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->sam3==0.1.0) (0.7.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->sam3==0.1.0) (0.3.0)\n","Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->sam3==0.1.0) (3.0.3)\n","Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->sam3==0.1.0) (3.2.0)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->sam3==0.1.0) (0.10.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->sam3==0.1.0) (1.5.1)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook->sam3==0.1.0) (2.21.2)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook->sam3==0.1.0) (4.26.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->sam3==0.1.0) (1.17.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.12/dist-packages (from terminado>=0.8.3->notebook->sam3==0.1.0) (0.7.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook->sam3==0.1.0) (25.1.0)\n","Requirement already satisfied: async-lru>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->sam3==0.1.0) (2.1.0)\n","Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->sam3==0.1.0) (0.28.1)\n","Requirement already satisfied: jupyter-lsp>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->sam3==0.1.0) (2.3.0)\n","Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->sam3==0.1.0) (2.14.0)\n","Requirement already satisfied: jupyterlab-server<3,>=2.28.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->sam3==0.1.0) (2.28.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->sam3==0.1.0) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->sam3==0.1.0) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->sam3==0.1.0) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->sam3==0.1.0) (2026.1.4)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm>=1.0.17->sam3==0.1.0) (1.14.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->sam3==0.1.0) (0.5.1)\n","Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->sam3==0.1.0) (1.4.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->sam3==0.1.0) (4.12.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->sam3==0.1.0) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter->sam3==0.1.0) (0.16.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->sam3==0.1.0) (0.8.5)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->sam3==0.1.0) (25.4.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->sam3==0.1.0) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->sam3==0.1.0) (0.37.0)\n","Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->sam3==0.1.0) (0.30.0)\n","Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (0.12.0)\n","Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (0.5.4)\n","Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (7.7.0)\n","Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (1.9.0)\n","Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->sam3==0.1.0) (2.17.0)\n","Requirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->sam3==0.1.0) (0.13.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm>=1.0.17->sam3==0.1.0) (1.3.0)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->sam3==0.1.0) (2.0.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert->jupyter->sam3==0.1.0) (2.8.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->sam3==0.1.0) (2.23)\n","Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (4.0.0)\n","Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (0.1.4)\n","Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (0.1.1)\n","Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (1.5.1)\n","Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (20.11.0)\n","Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (3.0.0)\n","Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (1.1.0)\n","Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (1.3.0)\n","Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (25.10.0)\n","Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (1.3.1)\n","Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (1.4.0)\n","Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->sam3==0.1.0) (2025.3)\n","Building wheels for collected packages: sam3\n","  Building editable for sam3 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sam3: filename=sam3-0.1.0-0.editable-py3-none-any.whl size=15413 sha256=29004d4c38cf2f061338d2c10ec541667978190abdfada7395569635d161d2c1\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-s3psnu20/wheels/dd/85/0d/50f71564a220f942d76bb9b370d0bd2a76e6e4fe8108c3cf67\n","Successfully built sam3\n","Installing collected packages: sam3\n","  Attempting uninstall: sam3\n","    Found existing installation: sam3 0.1.0\n","    Uninstalling sam3-0.1.0:\n","      Successfully uninstalled sam3-0.1.0\n","Successfully installed sam3-0.1.0\n","/content\n"]}]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"],"metadata":{"id":"nMaVLEj3g9_l","executionInfo":{"status":"ok","timestamp":1769496119432,"user_tz":-60,"elapsed":404,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.insert(0, '/content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM 3/sam3_repo')\n","from sam3 import build_sam3_image_model\n","from sam3.model.sam3_image_processor import Sam3Processor\n","import os\n","\n","bpe_path = \"/content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM 3/sam3_repo/sam3/assets/bpe_simple_vocab_16e6.txt.gz\"\n","model = build_sam3_image_model(bpe_path=bpe_path)\n","processor = Sam3Processor(model, confidence_threshold=0.5)"],"metadata":{"id":"80RH7FS2g8FU","colab":{"base_uri":"https://localhost:8080/","height":346},"executionInfo":{"status":"error","timestamp":1769496251722,"user_tz":-60,"elapsed":36,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}},"outputId":"7fd72208-6830-4526-94dc-c7d9174f90ab"},"execution_count":7,"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1721530469.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbpe_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM 3/sam3_repo/sam3/assets/bpe_simple_vocab_16e6.txt.gz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_sam3_image_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbpe_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSam3Processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM 3/sam3_repo/sam3/model_builder.py\u001b[0m in \u001b[0;36mbuild_sam3_image_model\u001b[0;34m(bpe_path, device, eval_mode, checkpoint_path, load_from_HF, enable_segmentation, enable_inst_interactivity, compile)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;31m# Create visual components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0mcompile_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"default\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcompile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     vision_encoder = _create_vision_backbone(\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mcompile_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_inst_interactivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_inst_interactivity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m     )\n","\u001b[0;32m/content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM 3/sam3_repo/sam3/model_builder.py\u001b[0m in \u001b[0;36m_create_vision_backbone\u001b[0;34m(compile_mode, enable_inst_interactivity)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;34m\"\"\"Create SAM3 visual backbone with ViT and neck.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;31m# Position encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m     \u001b[0mposition_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_position_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecompute_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1008\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m     \u001b[0;31m# ViT backbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0mvit_backbone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mViT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_vit_backbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM 3/sam3_repo/sam3/model_builder.py\u001b[0m in \u001b[0;36m_create_position_encoding\u001b[0;34m(precompute_resolution)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_create_position_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecompute_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;34m\"\"\"Create position encoding for visual backbone.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     return PositionEmbeddingSine(\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mnum_pos_feats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM 3/sam3_repo/sam3/model/position_encoding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_pos_feats, temperature, normalize, scale, precompute_resolution)\u001b[0m\n\u001b[1;32m     47\u001b[0m             ]\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprecompute_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;31m# further clone and detach it in the cache (just to be safe)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    401\u001b[0m             )\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cuda_getDeviceCount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             raise AssertionError(\n","\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"cja3S3FOfAoG","executionInfo":{"status":"ok","timestamp":1769496248103,"user_tz":-60,"elapsed":1862,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"outputs":[],"source":["import os\n","import glob\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import nibabel as nib\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, f1_score, jaccard_score\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["## Tumor Dataset"],"metadata":{"id":"-ooj2wXUKjNT"}},{"cell_type":"code","source":["class BraTSDataset(Dataset):\n","    \"\"\"\n","    BraTS dataset for linear probing.\n","    Returns: 2D slices from 3D volumes with corresponding segmentation masks\n","    \"\"\"\n","    def __init__(self, data_root, modality='t1ce', slice_range=(50, 130),\n","                 normalize=True, img_size=1008):\n","        \"\"\"\n","        Args:\n","            data_root: Path to BraTS training data directory\n","            modality: Which MRI modality to use ('flair', 't1', 't1ce', 't2')\n","            slice_range: (min, max) slice indices to use from each volume\n","            normalize: Whether to normalize images to [0, 1]\n","            img_size: Size to resize images to (SAM3 expects 1008x1008)\n","        \"\"\"\n","        self.data_root = data_root\n","        self.modality = modality\n","        self.slice_range = slice_range\n","        self.normalize = normalize\n","        self.img_size = img_size\n","\n","        # Find all patient directories\n","        self.patient_dirs = sorted(glob.glob(os.path.join(data_root, \"BraTS20_Training_*\")))\n","        print(f\"Found {len(self.patient_dirs)} patients\")\n","\n","        # Build index of all valid slices\n","        self.slice_index = []\n","        for patient_dir in self.patient_dirs:\n","            patient_id = os.path.basename(patient_dir)\n","            # Check if files exist\n","            img_path = os.path.join(patient_dir, f\"{patient_id}_{modality}.nii\")\n","            seg_path = os.path.join(patient_dir, f\"{patient_id}_seg.nii\")\n","\n","            if os.path.exists(img_path) and os.path.exists(seg_path):\n","                # Add each slice in range\n","                for slice_idx in range(slice_range[0], slice_range[1]):\n","                    self.slice_index.append((patient_dir, patient_id, slice_idx))\n","\n","        print(f\"Total slices: {len(self.slice_index)}\")\n","\n","    def __len__(self):\n","        return len(self.slice_index)\n","\n","    def __getitem__(self, idx):\n","        patient_dir, patient_id, slice_idx = self.slice_index[idx]\n","\n","        # Load image and segmentation\n","        img_path = os.path.join(patient_dir, f\"{patient_id}_{self.modality}.nii\")\n","        seg_path = os.path.join(patient_dir, f\"{patient_id}_seg.nii\")\n","\n","        img_nib = nib.load(img_path)\n","        seg_nib = nib.load(seg_path)\n","\n","        img_data = img_nib.get_fdata()\n","        seg_data = seg_nib.get_fdata()\n","\n","        # Extract 2D slice\n","        img_slice = img_data[:, :, slice_idx]\n","        seg_slice = seg_data[:, :, slice_idx]\n","\n","        # Normalize image\n","        if self.normalize:\n","            img_slice = (img_slice - img_slice.min()) / (img_slice.max() - img_slice.min() + 1e-8)\n","\n","        # Convert to RGB (SAM expects 3 channels)\n","        img_slice_rgb = np.stack([img_slice, img_slice, img_slice], axis=-1)\n","\n","        # Binary segmentation: any tumor (label > 0) vs background (label == 0)\n","        seg_binary = (seg_slice > 0).astype(np.float32)\n","\n","        # Convert to tensors\n","        img_tensor = torch.from_numpy(img_slice_rgb).float().permute(2, 0, 1)  # C, H, W\n","        seg_tensor = torch.from_numpy(seg_binary).float()  # H, W\n","\n","        # Resize\n","        img_tensor = torch.nn.functional.interpolate(\n","            img_tensor.unsqueeze(0),\n","            size=(self.img_size, self.img_size),\n","            mode='bilinear',\n","            align_corners=False\n","        ).squeeze(0)\n","\n","        seg_tensor = torch.nn.functional.interpolate(\n","            seg_tensor.unsqueeze(0).unsqueeze(0),\n","            size=(self.img_size, self.img_size),\n","            mode='nearest'\n","        ).squeeze(0).squeeze(0).long()\n","\n","        return img_tensor, seg_tensor"],"metadata":{"id":"YjknDATHfXz9","executionInfo":{"status":"aborted","timestamp":1769496228045,"user_tz":-60,"elapsed":48264,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Linear probe classifier with SAM 3 frozen features"],"metadata":{"id":"K3_-JFOjKmIo"}},{"cell_type":"code","source":["class LinearProbe(nn.Module):\n","    \"\"\"\n","    Simple linear classifier for segmentation.\n","    Takes frozen SAM3 features and predicts per-pixel class labels.\n","    \"\"\"\n","    def __init__(self, feature_dim=256, num_classes=2, feature_spatial_size=(72, 72)):\n","        \"\"\"\n","        Args:\n","            feature_dim: Dimension of SAM3 features (256 for SAM3)\n","            num_classes: Number of segmentation classes (2 for binary)\n","            feature_spatial_size: Spatial resolution of SAM3 features (72x72)\n","        \"\"\"\n","        super().__init__()\n","        self.feature_dim = feature_dim\n","        self.num_classes = num_classes\n","        self.feature_spatial_size = feature_spatial_size\n","\n","        # Simple 1x1 convolution (equivalent to per-pixel linear classifier)\n","        self.classifier = nn.Conv2d(feature_dim, num_classes, kernel_size=1)\n","\n","    def forward(self, features):\n","        \"\"\"\n","        Args:\n","            features: [B, C, H, W] feature maps from SAM3 (e.g., [B, 256, 72, 72])\n","        Returns:\n","            logits: [B, num_classes, H, W] per-pixel class logits\n","        \"\"\"\n","        logits = self.classifier(features)\n","        return logits"],"metadata":{"id":"F8_v088AfctB","executionInfo":{"status":"aborted","timestamp":1769496228052,"user_tz":-60,"elapsed":45091,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Feature Extractor from SAM 3"],"metadata":{"id":"DpNTwB1nKrC6"}},{"cell_type":"code","source":["class SAM3FeatureExtractor:\n","    \"\"\"\n","    Extracts features from frozen SAM3 encoder.\n","    \"\"\"\n","    def __init__(self, sam3_model, device='cuda'):\n","        self.model = sam3_model\n","        self.device = device\n","\n","        if hasattr(self.model, 'backbone'):\n","            for param in self.model.backbone.parameters():\n","                param.requires_grad = False\n","            for name, p in sam3_model.named_parameters():\n","                p.requires_grad = False\n","            for name, p in sam3_model.named_parameters():\n","                if name.startswith(\"segmentation_head\"):\n","                    p.requires_grad = True\n","            print(\"Froze SAM3 backbone parameters\")\n","        else:\n","            raise AttributeError(\"SAM3 model doesn't have 'backbone' attribute\")\n","\n","        self.model.eval()\n","        self.model.to(device)\n","\n","    @torch.no_grad()\n","    def extract_features(self, images, captions=None):\n","        \"\"\"\n","        Extract features from SAM3 backbone.\n","\n","        Args:\n","            images: [B, 3, H, W] input images\n","            captions: List of text captions (SAM3 is vision-language model)\n","        Returns:\n","            features: Tensor of visual features [B, C, H, W]\n","        \"\"\"\n","        # SAM3 requires captions (vision-language model)\n","        if captions is None:\n","            # Default caption for tumor segmentation\n","            batch_size = images.shape[0]\n","            captions = [\"brain tumor\"] * batch_size\n","\n","        # SAM3 backbone returns a dictionary of features\n","        backbone_out = self.model.backbone(images, captions)\n","\n","        # Extract the visual features from the dictionary\n","        if isinstance(backbone_out, dict):\n","            # SAM3 uses 'vision_features' key\n","            if 'vision_features' in backbone_out:\n","                return backbone_out['vision_features']\n","            # Fallback to other common keys\n","            for key in ['image_features', 'visual_features', 'features']:\n","                if key in backbone_out:\n","                    return backbone_out[key]\n","            # If none found, return first tensor\n","            for val in backbone_out.values():\n","                if isinstance(val, torch.Tensor) and len(val.shape) == 4:\n","                    return val\n","\n","        return backbone_out"],"metadata":{"id":"ewAmGsEyUYb_","executionInfo":{"status":"aborted","timestamp":1769496228055,"user_tz":-60,"elapsed":43177,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train"],"metadata":{"id":"3jRVkvX7KueN"}},{"cell_type":"code","source":["def train_linear_probe(\n","    train_loader,\n","    val_loader,\n","    feature_extractor,\n","    probe,\n","    num_epochs=20,\n","    lr=0.001,\n","    device='cuda'\n","):\n","    \"\"\"\n","    Train the linear probe on frozen SAM3 features.\n","    \"\"\"\n","    probe.to(device)\n","    optimizer = optim.Adam(probe.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    train_losses = []\n","    val_accuracies = []\n","    val_ious = []\n","\n","    for epoch in range(num_epochs):\n","        # Training phase\n","        probe.train()\n","        epoch_loss = 0\n","\n","        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n","        for images, masks in pbar:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            # Extract frozen features\n","            with torch.no_grad():\n","                features = feature_extractor.extract_features(images)\n","\n","            # Forward through probe\n","            logits = probe(features)\n","\n","            # Resize logits to match mask size if needed\n","            if logits.shape[-2:] != masks.shape[-2:]:\n","                logits = torch.nn.functional.interpolate(\n","                    logits, size=masks.shape[-2:], mode='bilinear', align_corners=False\n","                )\n","\n","            # Compute loss\n","            loss = criterion(logits, masks)\n","\n","            # Backward\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()\n","            pbar.set_postfix({'loss': loss.item()})\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        train_losses.append(avg_loss)\n","\n","        # Validation phase\n","        val_acc, val_iou = evaluate_probe(val_loader, feature_extractor, probe, device)\n","        val_accuracies.append(val_acc)\n","        val_ious.append(val_iou)\n","\n","        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Val Acc={val_acc:.4f}, Val IoU={val_iou:.4f}\")\n","\n","    return {\n","        'train_losses': train_losses,\n","        'val_accuracies': val_accuracies,\n","        'val_ious': val_ious\n","    }"],"metadata":{"id":"O11AsholUbmI","executionInfo":{"status":"aborted","timestamp":1769496228058,"user_tz":-60,"elapsed":41229,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate"],"metadata":{"id":"66mU0jizKv2W"}},{"cell_type":"code","source":["def evaluate_probe(data_loader, feature_extractor, probe, device='cuda'):\n","    \"\"\"\n","    Evaluate the linear probe on a dataset.\n","    \"\"\"\n","    probe.eval()\n","    all_preds = []\n","    all_targets = []\n","\n","    with torch.no_grad():\n","        for images, masks in tqdm(data_loader, desc=\"Evaluating\"):\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            # Extract features\n","            features = feature_extractor.extract_features(images)\n","\n","            # Predict\n","            logits = probe(features)\n","\n","            # Resize if needed\n","            if logits.shape[-2:] != masks.shape[-2:]:\n","                logits = torch.nn.functional.interpolate(\n","                    logits, size=masks.shape[-2:], mode='bilinear', align_corners=False\n","                )\n","\n","            preds = torch.argmax(logits, dim=1)\n","\n","            all_preds.append(preds.cpu().numpy().flatten())\n","            all_targets.append(masks.cpu().numpy().flatten())\n","\n","    all_preds = np.concatenate(all_preds)\n","    all_targets = np.concatenate(all_targets)\n","\n","    accuracy = accuracy_score(all_targets, all_preds)\n","    iou = jaccard_score(all_targets, all_preds, average='binary')\n","\n","    return accuracy, iou"],"metadata":{"id":"T3CArXkIUeYg","executionInfo":{"status":"aborted","timestamp":1769496228061,"user_tz":-60,"elapsed":38795,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plots"],"metadata":{"id":"vx87I6qBKxzL"}},{"cell_type":"code","source":["def plot_results(history, save_path='linear_probe_results.png'):\n","    \"\"\"\n","    Plot training history.\n","    \"\"\"\n","    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n","\n","    # Loss\n","    axes[0].plot(history['train_losses'])\n","    axes[0].set_title('Training Loss')\n","    axes[0].set_xlabel('Epoch')\n","    axes[0].set_ylabel('Loss')\n","    axes[0].grid(True)\n","\n","    # Accuracy\n","    axes[1].plot(history['val_accuracies'])\n","    axes[1].set_title('Validation Accuracy')\n","    axes[1].set_xlabel('Epoch')\n","    axes[1].set_ylabel('Accuracy')\n","    axes[1].grid(True)\n","\n","    # IoU\n","    axes[2].plot(history['val_ious'])\n","    axes[2].set_title('Validation IoU')\n","    axes[2].set_xlabel('Epoch')\n","    axes[2].set_ylabel('IoU')\n","    axes[2].grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig(save_path)\n","    print(f\"Results saved to {save_path}\")"],"metadata":{"id":"wgyFBkOgUg8a","executionInfo":{"status":"aborted","timestamp":1769496228063,"user_tz":-60,"elapsed":36969,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Parameters"],"metadata":{"id":"jtPeSrGPLAJC"}},{"cell_type":"code","source":["data_root = \"/content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM 3/data/tumor/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n","bpe_path = \"/content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM 3/sam3_repo/sam3/assets/bpe_simple_vocab_16e6.txt.gz\"\n","\n","batch_size = 8\n","num_epochs = 1\n","lr = 0.001\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","print(f\"Using device: {device}\")"],"metadata":{"id":"N458Zsj3Ukbm","executionInfo":{"status":"aborted","timestamp":1769496228108,"user_tz":-60,"elapsed":35265,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch, gc\n","\n","torch.cuda.empty_cache()\n","gc.collect()\n","torch.cuda.reset_peak_memory_stats()\n"],"metadata":{"id":"Hjz6b4g7W9cO","executionInfo":{"status":"aborted","timestamp":1769496228110,"user_tz":-60,"elapsed":34005,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load model"],"metadata":{"id":"_A8BN-zfK0Qj"}},{"cell_type":"code","source":["print(\"Loading SAM3 model...\")\n","sam3_model = build_sam3_image_model(bpe_path=bpe_path)\n","feature_extractor = SAM3FeatureExtractor(sam3_model, device=device)\n","print(\"SAM3 encoder frozen!\")"],"metadata":{"id":"BJ-MYPwiUpZY","executionInfo":{"status":"aborted","timestamp":1769496228112,"user_tz":-60,"elapsed":31926,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sum(p.numel() for p in feature_extractor.model.parameters() if p.requires_grad)"],"metadata":{"id":"df7DdUmGCEhh","executionInfo":{"status":"aborted","timestamp":1769496228113,"user_tz":-60,"elapsed":30947,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load and prepare dataset"],"metadata":{"id":"LPfRtB7CK16o"}},{"cell_type":"code","source":["print(\"Loading datasets...\")\n","full_dataset = BraTSDataset(data_root, modality='t1ce')"],"metadata":{"id":"2hSSma3FUwjH","executionInfo":{"status":"aborted","timestamp":1769496228121,"user_tz":-60,"elapsed":29626,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_size = int(0.8 * len(full_dataset))\n","val_size = len(full_dataset) - train_size\n","train_dataset, val_dataset = torch.utils.data.random_split(\n","    full_dataset, [train_size, val_size]\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","print(f\"Train: {len(train_dataset)} slices, Val: {len(val_dataset)} slices\")"],"metadata":{"id":"9y0r_b3gU1zZ","executionInfo":{"status":"aborted","timestamp":1769496228124,"user_tz":-60,"elapsed":28198,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_dim = 256\n","probe = LinearProbe(feature_dim=feature_dim, num_classes=2)"],"metadata":{"id":"Zcl6wlnrVVmY","executionInfo":{"status":"aborted","timestamp":1769496228126,"user_tz":-60,"elapsed":25900,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Main training loop"],"metadata":{"id":"9fH13A1KK5b_"}},{"cell_type":"code","source":["history = train_linear_probe(\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    feature_extractor=feature_extractor,\n","    probe=probe,\n","    num_epochs=num_epochs,\n","    lr=lr,\n","    device=device\n",")"],"metadata":{"id":"7HeiDzV0XS5e","executionInfo":{"status":"aborted","timestamp":1769496228126,"user_tz":-60,"elapsed":24801,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Results"],"metadata":{"id":"-10cy1LYK7R8"}},{"cell_type":"code","source":["plot_results(history)"],"metadata":{"id":"jn1F9J--XWGe","colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"status":"error","timestamp":1769496156879,"user_tz":-60,"elapsed":6,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}},"outputId":"7df92e80-d3da-48d2-b6a9-d40482733ee2"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'plot_results' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-370869387.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'plot_results' is not defined"]}]},{"cell_type":"code","source":["torch.save(probe.state_dict(), 'linear_probe_brats.pth')\n","print(\"Linear probe saved!\")"],"metadata":{"id":"yPjaZQHQXa58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -l /content/drive/MyDrive/MSc_Thesis_Neuroimaging/SAM\\ 3/data/tumor/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FDv_pCkLTdMq","executionInfo":{"status":"ok","timestamp":1769447767229,"user_tz":-60,"elapsed":139,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}},"outputId":"78b39fee-8d41-43f5-d350-fdb6f6c1d6cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 78484\n","-rw------- 1 root root 17858880 Jan 26 15:36 BraTS20_Training_001_flair.nii\n","-rw------- 1 root root  8930976 Jan 26 15:36 BraTS20_Training_001_seg.nii\n","-rw------- 1 root root 17858880 Jan 26 15:36 BraTS20_Training_001_t1ce.nii\n","-rw------- 1 root root 17858880 Jan 26 15:36 BraTS20_Training_001_t1.nii\n","-rw------- 1 root root 17858880 Jan 26 15:36 BraTS20_Training_001_t2.nii\n"]}]},{"cell_type":"code","source":["print(\"\\nDetermining SAM3 feature dimensions...\")\n","dummy_image = torch.randn(1, 3, 1008, 1008).to(device)\n","dummy_captions = [\"tumor\"]\n","\n","with torch.no_grad():\n","    sam3_model.eval()\n","    features = sam3_model.backbone(dummy_image, dummy_captions)\n","\n","    print(f\"Feature output type: {type(features)}\")\n","\n","    if isinstance(features, dict):\n","        print(f\"Feature keys: {list(features.keys())}\")\n","        for key, val in features.items():\n","            if isinstance(val, torch.Tensor):\n","                print(f\"  {key}: shape {val.shape}\")\n","\n","        possible_keys = ['vision_features', 'image_features', 'visual_features', 'features', 'vision_embedding']\n","        feature_tensor = None\n","        feature_key = None\n","        for key in possible_keys:\n","            if key in features and isinstance(features[key], torch.Tensor):\n","                feature_tensor = features[key]\n","                feature_key = key\n","                print(f\"\\nUsing '{key}' as main features: {feature_tensor.shape}\")\n","                break\n","\n","        if feature_tensor is None:\n","            for key, val in features.items():\n","                if isinstance(val, torch.Tensor):\n","                    feature_tensor = val\n","                    feature_key = key\n","                    print(f\"\\nUsing '{key}' as main features: {feature_tensor.shape}\")\n","                    break\n","\n","        if feature_tensor is not None:\n","            if len(feature_tensor.shape) == 4:  # [B, C, H, W]\n","                feature_dim = feature_tensor.shape[1]\n","                print(f\"\\n Feature dimension: {feature_dim}\")\n","                print(f\" Feature spatial size: {feature_tensor.shape[2]}x{feature_tensor.shape[3]}\")\n","            elif len(feature_tensor.shape) == 3:  # [B, N, C] sequence format\n","                feature_dim = feature_tensor.shape[2]\n","                print(f\"\\n Feature dimension: {feature_dim}\")\n","                print(f\" Sequence length: {feature_tensor.shape[1]}\")\n","\n","    elif isinstance(features, torch.Tensor):\n","        print(f\"Feature shape: {features.shape}\")\n","        if len(features.shape) == 4:\n","            feature_dim = features.shape[1]\n","            print(f\"\\n Feature dimension: {feature_dim}\")\n","            print(f\" Feature spatial size: {features.shape[2]}x{features.shape[3]}\")\n","\n","print(f\"\\nSet feature_dim = {feature_dim}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"WPpYgFN0VezW","executionInfo":{"status":"error","timestamp":1769460211311,"user_tz":-60,"elapsed":348,"user":{"displayName":"Tamara Kostova","userId":"00889377672286813583"}},"outputId":"ec90cb16-b44f-41c4-aa2e-079c061520d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Determining SAM3 feature dimensions...\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 239277 has 14.73 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 318.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-353731201.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDetermining SAM3 feature dimensions...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdummy_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1008\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1008\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdummy_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"tumor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 239277 has 14.73 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 318.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}]}]}